name: megamolbart_finetune
do_training: True
do_testing: False
seed: 42
restore_from_path: /tmp/models/MegaMolBART_0_2_0.nemo

trainer:
  devices: 1
  num_nodes: 1
  precision: 16
  accelerator: gpu
  max_epochs: null
  max_steps: 1000
  log_every_n_steps: 10
  val_check_interval: 50
  limit_val_batches: 1
  limit_test_batches: 0
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  enable_checkpointing: False
  logger: False
  replace_sampler_ddp: False

model:
  name: megamolbart_finetune_model
  global_batch_size: 8
  micro_batch_size: 8
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  resume_from_checkpoint: null
  pipeline_model_parallel_split_rank: 0

  make_vocab_size_divisible_by: 128
  pre_process: True
  post_process: True
  megatron_amp_O2: False
  seq_length: 512
  max_position_embeddings: 512
  num_layers: 3
  hidden_size: 768
  ffn_hidden_size: 3072
  num_attention_heads: 12
  init_method_std: 0.02
  hidden_dropout: 0.1
  attention_dropout: 0.1
  kv_channels: null
  apply_query_key_layer_scaling: True
  layernorm_epsilon: 1e-5
  persist_layer_norm: True
  gradient_as_bucket_view: True
  bias_gelu_fusion: True
  masked_softmax_fusion: True
  bias_dropout_add_fusion: True
  bias: True
  normalization: 'layernorm'
  encoder_arch: 'transformer'
  decoder_arch: 'transformer'
  activation: 'gelu'
  headscale: False
  share_word_embeddings: True
  share_decoder_tokens_head_embeddings: False

  tokenizer:
    library: 'regex'
    type: null
    model: /opt/nvidia/nemo_chem/models/vocab/megamolbart.model
    vocab_file: /opt/nvidia/nemo_chem/models/vocab/megamolbart.vocab
    merge_file: null

  data:
    links_file: null
    dataset_path: /workspace/zinc_example_data
    dataset:
      train: zinc_data_finetune_train
      val: zinc_data_finetune_valid
      test: zinc_data_finetune_test
    encoder_augment: True
    encoder_mask: True
    decoder_augment: True
    decoder_mask: False
    mask_scheme: span
    mask_prob: 0.15
    span_lambda: 3.0
    micro_batch_size: 8
    #num_workers: 10
    num_enumerations: 3
    dataloader_type: single
    dataset_format: csv
    canonicalize_input: true

  optim:
    name: fused_adam
    lr: 1.0
    betas:
      - 0.9
      - 0.999
    eps: 1e-8
    weight_decay: 0.01
    sched:
      name: NoamAnnealing
      d_model: 768
      warmup_steps: 100
      max_steps: 1000
      min_lr: 1e-5

exp_manager:
  name: megamolbart_finetune
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  create_checkpoint_callback: True
  checkpoint_callback_params:
    save_top_k: 1
    monitor: val_molecular_accuracy
    mode: max
    save_last: True
    always_save_nemo: True
    filename: 'finetune--{val_molecular_accuracy:.2f}-{step}'
    model_parallel_size: 1
  exp_dir: /workspace/results/finetune_logs
  create_tensorboard_logger: True
  create_wandb_logger: False
